\section {PlaceRecognizer Application}
After crawling the images and training a model, this section explains how we used these prerequisites to create an Android application that is capable of classifying images taken with the camera.

\subsection {CNNDroid Integration/Image Classifier}
The \lstinline[language=Java]{ImageClassifier} class is the connection from CNNDroid to our application. It handles the creation of the CNNDroid classifier object as an asynchronous background task, the classification of a bitmap input and provides a function to output the accuracy of the result.\\
During creation of the CNNDroid object, the path to the model files (the layers in MessagePack format, the definition file and the labels file) has to be specified. The location of the files can be specified by setting the \texttt{rootpath} variable, which should point to the folder containing these files. Additionally, the name of the definition file has to be specified inside the background task.\\
The files have to be copied onto the Android device manually. If an emulator is used, the folder can simply dragged onto the emulated phone. Per default it is then saved at \texttt{/sdcard/Download/}. Old files are automatically overwritten.\\
If a physical phone is used, the files can either be copied using the operating systems file explorer (if supported) or using the command line tool \texttt{adb}\footnote{Android Debug Bridge}, which comes with Android Studio. The command for copying from the host device to the android phone is
\begin{center}
    \texttt{\$ adb push `/local/path/' `/path/on/phone'}
\end{center}

The \lstinline[language=Java]{classifyImage} function takes a bitmap image as input. This image is converted into an array of shape \texttt{[n][numColorChannels][width][height]} by iterating over every pixel and splitting its RGB-value into its components. The dimensions of the array are defined as follows:
\begin{itemize}
    \item{n - Number of images to be classified. Because we currently only classify one image at a time, this is always 1.}
    \item{numColorChannels - If we classify on colored images, this is 3, with the first channel being the red, the second one the green and the last one the blue component. If the input image should be in grayscale there is only one channel and this part has to be adapted.}
    \item{width - width of the scaled input image}
    \item{height - height of the scaled input image}
\end{itemize}

The taken image is automatically scaled to fit the dimensions that the model expects. If the image dimensions of the trained model are changed, the variables \texttt{imageWidth} and \texttt{imageHeight} have to be changed as well.\\
Optionally, it is possible to use a mean file for improved classification results. The mean file has to be put into the folder specified by \texttt{rootpath} and the relevant code parts have to be uncommented. A python script that transforms a \texttt{.mean} file, for example generated by caffe, into MessagePack format is provided as well, called \texttt{binaryproto\_to\_msg.py} in our projects \texttt{utils} directory.\\
The resulting input array is passed to the \texttt{CNNDroid::compute} function, which returns an array with shape \texttt{[n][numClasses]}. \lstinline[language=Java]{input[0][0]} contains the predicted probability of the input image being the zeroth class.\\
The output array can now be passed to our \texttt{ImageClassifier::accuracy} function, which maps the accuracy to the corresponding label for the top k results. The returned value is a string showing this mapping, sorted from the most to least probable class.\\
Per default the \texttt{classify} function returns a \texttt{Classification} object, which contains the ID, probability and label of the best match found. The values can be extracted with \texttt{Classification::get\_id()}, \texttt{Classification::get\_label()} and \texttt{Classification::get\_probability()}.

\subsection {Real-Time Frame Capture}
The \texttt{CameraFrameCapture} class is based on the Camera2 Android API. It is a specialized version of the Camera2Basic example\footnote{https://github.com/googlesamples/android-Camera2Basic}.\\
The most important part is the preview of the camera image and the processing of each frame that comes out of it. For that, we create a CaptureRequest inside the \texttt{createCameraPreviewSession} function. This CaptureRequest needs to be rendered in order to access the image data. This is done by calling
\begin{lstlisting}[language=Java, basicstyle=\scriptsize]
    mPreviewRequestBuilder.addTarget(surface);
    mPreviewRequestBuilder.addTarget(mImageReader.getSurface());
\end{lstlisting}

The first part makes the image visible inside the application. The second part passes the image to our imageReader.\\
The imageReader has an \texttt{OnImageAvailableListener} callback, which is executed every time the imageReader receives a new image (i.e. every frame). The image provided by the API is in YUV-format\footnote{https://en.wikipedia.org/wiki/YUV}. To process the pixels, we need to convert them into RGB. This is done by saving the byte array into a YuvImage object, which has the ability to convert itself into jpeg, which contains RGB-values and can be decoded into a bitmap object. This bitmap object can now be passed to our classifier.\\
Right now, we only classify the image and use the debug information to evaluate the result. A function \texttt{getCurrentFrame} is also provided, which is a getter function for the bitmap image, so that other classes, like the \texttt{MainActivity} class or the \texttt{ImageClassifier} can use the image.

\subsection {GPS Logger}
The \texttt{GPSTracker} class provides the possibility to use the smart phones GPS receiver to get the current location. The \texttt{GPSTracker::getLocation()} function handles permission requests to the user, who has to allow our application to use location services and uses Androids \texttt{LocationManager} to receive the current location.\\
The return value is a \texttt{Location} object, which holds a lot of information about the GPS result, like latitude and longitude, heading, altitude, accuracy and speed. Because we only care about the position for now, we only provided a \texttt{GPSTracker::getLatitude()} and \texttt{GPSTracker::getLongitude()} function, which return their respective value as a decimal value.

\subsection {Wikipedia Parser}\label{wikipedia_parser}
When the CNNDroid classifier calculated an image class, we now want to get information about the place. Wikipedia is one of the largest source of information and has a very big and active community, which regularly updates the articles. Also, the list of features we can grab from Wikipedia is long: Descriptions, images, literature, etc..
Additionally, compared to other APIs, the Wikipedia API presents three main advantages:
\begin{itemize}
    \item{The information is provided by making a simple URL request}
    \item{There use no usage limitations}
    \item{It provides an easily parsable JSON response}
\end{itemize}
The Wikipedia API is composed of a principal URL which is \texttt{https://en.wikipedia.org/w/api.php}. This lets your change the language of the response easily by changing the subdomain of the URL. For example, to get a French JSON content, you can call \texttt{https://fr.wikipedia.org/w/api.php}.\\
This principal URL will be enriched with parameters, like the \texttt{format} (xml, json, html) or the \texttt{titles} to get information about a specific page. The second part of this section will clarify which parameters we use to call the Wikipedia API.\\\\

Our application uses two classes:
\subsubsection{HttpHandler.java}
The first class, "HttpHandler.java" creates a temporary byte array from the URL and also catches errors to check if the request is correct and returned a value.
At first we use the http GET method to open a connection.
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    try {
        URL url = new URL(reqUrl);
        HttpURLConnection conn = (HttpURLConnection)
        url.openConnection();
        conn.setRequestMethod("GET");
\end{lstlisting}

Then we will catch errors, for the following cases:Incorrect input URL, Protocol exception, Input/Output error during the use of the InputStream() function, or if the response is an empty array of bytes.
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    } catch (MalformedURLException e) {
            Log.e(TAG, "MalformedURLException: " + e.getMessage());
        } catch (ProtocolException e) {
            Log.e(TAG, "ProtocolException: " + e.getMessage());
        } catch (IOException e) {
            Log.e(TAG, "IOException: " + e.getMessage());
        } catch (Exception e) {
            Log.e(TAG, "Exception: " + e.getMessage());
\end{lstlisting}

\subsubsection{GetWiki.java}
This is a public class using an asynchronous task. Our classifier will map a name to the image taken by the user, for example "Brandenburg Gate" or "Fernsehturm Berlin". This label will be used as the input string variable for our GetWiki class.\\
First, the class is called from the mainActivity class, with the following statement:

\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    wikipediaInfos = new GetWiki().execute("classLabel")
\end{lstlisting}

"classLabel" is a string variable, which is dynamically update with the class given by the classifier. So it is also important to give a reliable class label regarding Wikipedia when the model is trained, because this label will be used as a parameter in the URL which is called:
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    String urlTitle = strings[0];
    String url = "https://en.wikipedia.org/w/api.php?" +
                 "format=json&action=query&prop=extracts&exintro=&explaintext=&titles=" +
                 urlTitle;
\end{lstlisting}

The response will be a JSON array that we can parse to extract the information we need. An example of a response looks like the following:
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    {"batchcomplete":"","query":{"pages":{"156604":{"pageid":156604,"ns":0,"title": "Brandenburg Gate","extract":"The Brandenburg Gate (German: Brandenburger Tor) is an 18th-century neoclassical monument in Berlin, and one of the best-known landmarks of Germany. It is built on the site of a former city gate that marked the start of the road from Berlin to the town of Brandenburg an der Havel....\n"}}}}
\end{lstlisting}
We truncated the response to keep only the parts we actually use, like the title and the description. The response contains of several JSON objects. We use the JsonObject package provided in the Android API 25, to parse the JSON response. In our example, we would like to get all informations contained in the \texttt{156606:\{} object:
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    JSONObject jsonObj = new JSONObject(jsonStr);
    JSONObject query = jsonObj.getJSONObject("query");
    JSONObject pages = query.getJSONObject("pages");
    Iterator<String> keys = pages.keys();
    String pageId= keys.next();
    JSONObject page = pages.getJSONObject(pageId);
    String title = page.getString("title");
    String extract = page.getString("extract");
\end{lstlisting}
First, we put the response into a JsonObject variable. Then we can use the string parameters to navigate through the JSON file. If we have an object that is not static, like the ID of the page, we use an Iterator \texttt{keys} which will return the value for this JSON Object. In the example, it will give us the pageId (156606) as a string. Then we just have to say that we want to go to the next value of the JSON array using keys.next().\\
Finally, we get the string value of the response and store them into variables.

\subsection{Text to speech}
To enhance the user experience of our app, as it is used to recognize famous places, we had a look into how to allow the user to play an audio description of the places description while they are watching it. The Android SDK provides a useful package for this:
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    import android.speech.tts.TextToSpeech
\end{lstlisting}
which has built in methods to parse a string value and read it as an audio track. It can easily be implemented with few lines of code, following these steps:\\\\

First, declare a TTS (text to speech) object, and set the attributes. Many attributes can be changed, like the language, the speed of the audio or the voice type. The following snippet shows how to create a textToSpeech object talking UK english.
\begin{lstlisting}[language=Java, basicstyle=\scriptsize]
    t1=new TextToSpeech(getApplicationContext(), new TextToSpeech.OnInitListener() {
                @Override
                public void onInit(int status) {
                    if(status != TextToSpeech.ERROR) {
                        t1.setLanguage(Locale.UK);
                    }
                }
            });
\end{lstlisting}

We can now pass a string, which should be converted into audio, to this object. In the following snippet we get the Wikipedia description of Brandenburg Gate from our Wikipedia API parser (see Section \ref{wikipedia_parser}).
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    String toSpeak = null;
    try {
    toSpeak = new GetWiki().execute("Brandenburg_Gate").get().description;
        } catch (InterruptedException e) {
          e.printStackTrace();
        } catch (ExecutionException e) {
          e.printStackTrace();
        }
\end{lstlisting}

Finally, you can use the speak() method from your object, with the queue mode (\texttt{QUEUE FLUSH}), which means that media to be played is dropped and replaced by the new entry each time you call this method.

\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    t1.speak(toSpeak, TextToSpeech.QUEUE_FLUSH, null , "");
\end{lstlisting}

\subsection{Firebase API}
Firebase is a Google service which provides a realtime database and backend as a service. The service is very well integrated into Android. The data is stored on Firebases cloud and the Firebase SDK for android comes up with several method to store and synchronize data in real time.

\subsubsection{How we use it}

First, we declare the Firebase service in the \texttt{build.gradle} file.
\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    compile 'com.google.firebase:firebase-core:10.0.1'
    compile 'com.google.firebase:firebase-database:10.0.1'
\end{lstlisting}

Then, like the GooglePlaceAPI, we need to setup an object of type database to start the connection with our database instance.

\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    private FirebaseAuth mAuth;
    private FirebaseAuth.AuthStateListener mAuthListener;
    DatabaseReference database = FirebaseDatabase.getInstance().getReference();
\end{lstlisting}

The Firebase structure uses paths to store the data. For example, you will have the top level path "users", which contains folders for each user of the app. Inside these directories the features are stored. To store or read data, the method will look like this:

\begin{lstlisting}[language=XML, basicstyle=\scriptsize]
    database.child("users").child(userId).child("places").push("location");
\end{lstlisting}
You need to define the levels you want to have access to. Here we first access the "users" node, then getting the current user ID, and we upload the locations details of his visit into the "places" node.

\subsection{How to setup project in Android Studio}
The project should work without any additional work in Android Studio. Simply import it into the IDE, setup an Android emulator or plug in an android phone running at least Android SDK 24.0 and press the ``Run'' button.